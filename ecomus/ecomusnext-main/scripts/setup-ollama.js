#!/usr/bin/env node

/**
 * Script de configuration Ollama pour Ecomus SaaS
 * Ce script configure et installe les mod√®les IA n√©cessaires pour le chatbot
 */

const { exec } = require('child_process');
const fs = require('fs');
const path = require('path');

console.log('üöÄ Configuration d\'Ollama pour Ecomus SaaS...\n');

// Configuration des mod√®les IA
const AI_MODELS = [
  {
    name: 'llama3.2',
    description: 'Mod√®le principal pour le chatbot e-commerce',
    size: '2B'
  },
  {
    name: 'codellama',
    description: 'Mod√®le pour assistance technique',
    size: '7B'
  }
];

// V√©rification de l'installation d'Ollama
function checkOllamaInstallation() {
  return new Promise((resolve, reject) => {
    exec('ollama --version', (error, stdout, stderr) => {
      if (error) {
        console.log('‚ùå Ollama n\'est pas install√©');
        console.log('üì• Installation d\'Ollama...');
        
        // Installation d'Ollama sur Linux/Mac
        exec('curl -fsSL https://ollama.com/install.sh | sh', (installError, installStdout, installStderr) => {
          if (installError) {
            reject('Erreur lors de l\'installation d\'Ollama: ' + installError.message);
          } else {
            console.log('‚úÖ Ollama install√© avec succ√®s');
            resolve(true);
          }
        });
      } else {
        console.log('‚úÖ Ollama est d√©j√† install√©:', stdout.trim());
        resolve(true);
      }
    });
  });
}

// D√©marrage du service Ollama
function startOllamaService() {
  return new Promise((resolve, reject) => {
    console.log('üîÑ D√©marrage du service Ollama...');
    
    exec('ollama serve', { detached: true }, (error, stdout, stderr) => {
      if (error && !error.message.includes('address already in use')) {
        reject('Erreur lors du d√©marrage d\'Ollama: ' + error.message);
      } else {
        console.log('‚úÖ Service Ollama d√©marr√©');
        resolve(true);
      }
    });
    
    // Attendre que le service d√©marre
    setTimeout(() => resolve(true), 3000);
  });
}

// Installation des mod√®les IA
function installModels() {
  return new Promise(async (resolve, reject) => {
    console.log('üì¶ Installation des mod√®les IA...\n');
    
    for (const model of AI_MODELS) {
      try {
        console.log(`‚¨áÔ∏è  T√©l√©chargement du mod√®le ${model.name} (${model.size})...`);
        console.log(`   Description: ${model.description}`);
        
        await new Promise((modelResolve, modelReject) => {
          exec(`ollama pull ${model.name}`, (error, stdout, stderr) => {
            if (error) {
              console.log(`‚ùå √âchec du t√©l√©chargement de ${model.name}: ${error.message}`);
              modelReject(error);
            } else {
              console.log(`‚úÖ Mod√®le ${model.name} install√© avec succ√®s\n`);
              modelResolve();
            }
          });
        });
        
      } catch (error) {
        console.log(`‚ö†Ô∏è  Avertissement: Impossible d'installer ${model.name}`);
      }
    }
    
    resolve(true);
  });
}

// Cr√©ation du fichier de configuration
function createConfigFile() {
  const config = {
    ollama: {
      baseUrl: 'http://localhost:11434',
      models: AI_MODELS.map(m => m.name),
      timeout: 30000
    },
    huggingface: {
      apiKey: process.env.HUGGINGFACE_API_KEY || 'YOUR_HUGGING_FACE_API_KEY',
      baseUrl: 'https://api-inference.huggingface.co'
    },
    chat: {
      maxTokens: 2048,
      temperature: 0.7,
      contextWindow: 4096
    }
  };
  
  const configPath = path.join(__dirname, '..', 'config', 'ai-config.json');
  const configDir = path.dirname(configPath);
  
  if (!fs.existsSync(configDir)) {
    fs.mkdirSync(configDir, { recursive: true });
  }
  
  fs.writeFileSync(configPath, JSON.stringify(config, null, 2));
  console.log('üìù Fichier de configuration cr√©√©:', configPath);
}

// Test de connexion
function testConnection() {
  return new Promise((resolve, reject) => {
    console.log('üß™ Test de connexion aux services IA...');
    
    exec('curl -s http://localhost:11434/api/tags', (error, stdout, stderr) => {
      if (error) {
        console.log('‚ùå Impossible de se connecter √† Ollama');
        reject(error);
      } else {
        try {
          const models = JSON.parse(stdout);
          console.log('‚úÖ Connexion Ollama r√©ussie');
          console.log('üìã Mod√®les disponibles:', models.models?.map(m => m.name).join(', ') || 'Aucun');
          resolve(true);
        } catch (parseError) {
          console.log('‚ö†Ô∏è  R√©ponse Ollama re√ßue mais format inattendu');
          resolve(true);
        }
      }
    });
  });
}

// Fonction principale
async function main() {
  try {
    await checkOllamaInstallation();
    await startOllamaService();
    await installModels();
    createConfigFile();
    await testConnection();
    
    console.log('\nüéâ Configuration termin√©e avec succ√®s!');
    console.log('\nüìã √âtapes suivantes:');
    console.log('1. Configurez votre cl√© API Hugging Face dans .env');
    console.log('2. D√©marrez votre application Next.js');
    console.log('3. Testez le chatbot IA dans l\'interface');
    console.log('\nüí° Le service Ollama fonctionne sur http://localhost:11434');
    
  } catch (error) {
    console.error('\n‚ùå Erreur lors de la configuration:', error);
    console.log('\nüîß Solutions possibles:');
    console.log('- V√©rifiez votre connexion internet');
    console.log('- Assurez-vous d\'avoir les droits d\'administration');
    console.log('- Red√©marrez le script apr√®s avoir r√©solu les probl√®mes');
    process.exit(1);
  }
}

// Ex√©cution du script
if (require.main === module) {
  main();
}

module.exports = {
  checkOllamaInstallation,
  startOllamaService,
  installModels,
  createConfigFile,
  testConnection
};
